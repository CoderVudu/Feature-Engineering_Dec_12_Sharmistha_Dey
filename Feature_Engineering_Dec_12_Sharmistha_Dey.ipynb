{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5v/V/bIkld7wWWXl7Vw0Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CoderVudu/Feature-Engineering_Dec_12_Sharmistha_Dey/blob/main/Feature_Engineering_Dec_12_Sharmistha_Dey.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q1. What is a parameter?"
      ],
      "metadata": {
        "id": "S_SXR95fIWsh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning, a parameter is a variable that the model learns from the data during training. Parameters define the model's behavior and are adjusted through optimization to minimize the error between predictions and actual outcomes. Examples include weights and biases in neural networks, coefficients in linear regression, and split thresholds in decision trees. These values are iteratively updated using algorithms like gradient descent to improve the model's performance. Parameters are distinct from hyperparameters, which are set before training and control the learning process. Together, parameters enable the model to generalize patterns from data for accurate predictions."
      ],
      "metadata": {
        "id": "rksBtMUSIgRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q2. What is correlation? What does negative correlation mean?"
      ],
      "metadata": {
        "id": "UBMXdenoIqEE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation measures the strength and direction of a relationship between two variables, typically represented by a correlation coefficient ranging from -1 to 1. In machine learning, it helps identify how changes in one feature relate to changes in another.\n",
        "\n",
        "Negative correlation means that as one variable increases, the other decreases, and vice versa. For example, a correlation of -0.8 indicates a strong inverse relationship. In machine learning, understanding negative correlations is important for feature selection and interpretation. Strongly correlated features (positive or negative) might lead to redundancy, while weakly correlated or independent features often provide more unique information for model training."
      ],
      "metadata": {
        "id": "LR70unymIu95"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q3. Define Machine Learning. What are the main components in Machine Learning?"
      ],
      "metadata": {
        "id": "_HTW8xsnI7H_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine Learning (ML) is a subset of artificial intelligence (AI) that enables systems to learn and improve from experience without being explicitly programmed. It focuses on developing algorithms that can analyze data, identify patterns, and make predictions or decisions based on new data.\n",
        "\n",
        "The main components of ML include:  \n",
        "1. Data: The foundation for training and testing models.  \n",
        "2. Algorithms: Methods for learning from data (e.g., supervised, unsupervised, and reinforcement learning).  \n",
        "3. Model: The representation of patterns learned by the algorithm.  \n",
        "4. Training: Process of teaching the model using labeled data.  \n",
        "5. Evaluation: Assessing model performance using metrics.  \n",
        "6. Deployment: Applying the model to real-world tasks."
      ],
      "metadata": {
        "id": "z6xABuR0I-0M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q4. How does loss value help in determining whether the model is good or not?"
      ],
      "metadata": {
        "id": "4kXvGaueJT-b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss value quantifies the difference between a model’s predictions and the actual target values, providing a measure of its performance. A low loss value indicates that the model's predictions are close to the true values, suggesting good performance. Conversely, a high loss value signals poor prediction accuracy, implying the need for improvement in the model. Monitoring the loss during training helps evaluate convergence; a steadily decreasing loss suggests the model is learning effectively. However, an overly low loss might indicate overfitting. Thus, loss value is crucial for assessing model quality and guiding optimization adjustments."
      ],
      "metadata": {
        "id": "zpslfm2aJY9n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q5. What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "0XvsNAvIJeYz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continuous and categorical variables are two types of data used in statistical analysis.\n",
        "\n",
        "Continuous variables can take any value within a range and are measured on a scale. Examples include height, weight, temperature, or time. They allow for fractional or decimal values and often represent quantities.\n",
        "\n",
        "Categorical variables represent distinct groups or categories and are qualitative in nature. Examples include gender, ethnicity, or types of vehicles. They can be further classified into nominal (no inherent order, e.g., colors) or ordinal (ordered categories, e.g., education levels).\n",
        "\n"
      ],
      "metadata": {
        "id": "AaKgZ1WfJirH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q6. How do we handle categorical variables in Machine Learning? What are the common techniques?"
      ],
      "metadata": {
        "id": "PagMdAUXKSmy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling categorical variables in machine learning involves converting them into numerical formats that models can process. Common techniques include label encoding, which assigns a unique integer to each category, suitable for ordinal data, and one-hot encoding, which creates binary columns for each category, preserving relationships for nominal data. Ordinal encoding is another method, assigning ordered numbers to categories with inherent order. For high cardinality, frequency encoding or target encoding may be used, representing categories based on their occurrence or target variable relationship. Advanced techniques like embedding layers in neural networks can also capture categorical data patterns effectively. Selection depends on the dataset and model."
      ],
      "metadata": {
        "id": "hGCl-6eTKWzC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q7. What do you mean by training and testing a dataset?"
      ],
      "metadata": {
        "id": "8aSRt3vSKph-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and testing a dataset refer to the process of evaluating a machine learning model. The dataset is split into two subsets: the training set and the testing set. The training set is used to teach the model by providing it with input data and corresponding output labels, enabling the model to learn patterns and relationships. After training, the testing set is used to assess the model’s performance on unseen data, determining its accuracy and generalization capability. This process ensures the model performs well not only on the data it was trained on but also in real-world scenarios."
      ],
      "metadata": {
        "id": "f9ebJ1jSKvZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q8. What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "423MNyE9K2r1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sklearn.preprocessing is a module in the scikit-learn library that provides tools for preprocessing and transforming data before it is used in machine learning models. It includes methods for standardizing, normalizing, encoding categorical variables, and scaling features to improve model performance. Common techniques include MinMaxScaler for scaling data to a specific range, StandardScaler for removing the mean and scaling data to unit variance, and OneHotEncoder for converting categorical variables into binary format. These preprocessing steps help ensure that data is in a consistent format and that machine learning algorithms can interpret and learn effectively from the provided data."
      ],
      "metadata": {
        "id": "UHGZ2WPmK6jB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q9. What is a Test set?"
      ],
      "metadata": {
        "id": "lRsOtQ8wLAEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A test set is a collection of data used to evaluate the performance of a trained machine learning model. It is separate from the training and validation datasets and consists of examples that the model has not encountered during training. The test set provides an unbiased assessment of how well the model generalizes to new, unseen data. By analyzing the results on the test set, researchers and practitioners can determine the model’s accuracy, precision, recall, or other performance metrics. A well-designed test set is crucial for ensuring the reliability and robustness of the model in real-world applications."
      ],
      "metadata": {
        "id": "AL6licq7LFID"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q10. How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?"
      ],
      "metadata": {
        "id": "WRMZPEgFLJzm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To split data for model fitting in Python, we commonly use `train_test_split` from the `sklearn.model_selection` library. This function divides the data into training and testing sets, typically with an 80-20 or 70-30 ratio. For example: `X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)`.  When approaching a Machine Learning problem, start by defining the problem and understanding the data. Preprocess the data, handling missing values and normalizing features. Select an appropriate model, train it on the data, and evaluate its performance using suitable metrics. Finally, fine-tune the model and test it on unseen data for generalization."
      ],
      "metadata": {
        "id": "54uR5X3bLOgs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q11. Why do we have to perform EDA before fitting a model to the data?"
      ],
      "metadata": {
        "id": "F-1pLvECOfEd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performing Exploratory Data Analysis (EDA) before fitting a model is crucial because it helps in understanding the data's structure, patterns, and potential issues. EDA allows us to identify outliers, missing values, and correlations between variables, which can significantly impact model performance. It also helps in selecting relevant features and deciding the appropriate model. By visualizing data distributions and relationships, EDA provides insights that guide data preprocessing steps like normalization or transformation. This step ensures that the data is clean, consistent, and appropriately prepared, improving the accuracy and efficiency of the modeling process."
      ],
      "metadata": {
        "id": "GZ2SIE01Oie_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q12. What is correlation?"
      ],
      "metadata": {
        "id": "z0xLIsqHOnso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation is a statistical measure that describes the relationship between two variables. It indicates how one variable changes in relation to another. A positive correlation means that as one variable increases, the other also tends to increase, while a negative correlation indicates that as one variable increases, the other decreases. Correlation is measured on a scale from -1 to 1, where 1 indicates a perfect positive correlation, -1 indicates a perfect negative correlation, and 0 indicates no correlation. It is important to note that correlation does not imply causation, meaning one variable does not necessarily cause the change in the other."
      ],
      "metadata": {
        "id": "q0Plp6wuOr6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q13. What does negative correlation mean?"
      ],
      "metadata": {
        "id": "RPxbdctVPoS8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative correlation refers to a relationship between two variables where, as one variable increases, the other decreases, or vice versa. In other words, when one variable goes up, the other tends to go down. This inverse relationship is measured on a scale from -1 to 0, where -1 indicates a perfect negative correlation, meaning the two variables are perfectly inversely related. A negative correlation suggests that the variables move in opposite directions, and changes in one can help predict the direction of change in the other. For example, as temperature drops, sales of ice cream may decrease."
      ],
      "metadata": {
        "id": "O_qxlMWmPsED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q14. How can you find correlation between variables in Python?"
      ],
      "metadata": {
        "id": "5FqaqT23P3LO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find the correlation between variables in Python, you can use the Pandas library, which provides a method called `corr()` to compute pairwise correlation of columns. First, import Pandas and load your data into a DataFrame. Then, call `data.corr()` to get the correlation matrix of all numerical variables. For a more specific correlation between two variables, you can use `data['column1'].corr(data['column2'])`. Alternatively, you can use libraries like NumPy for more customized correlation functions or visualize correlations using Seaborn's heatmap, which makes it easier to interpret complex correlation patterns."
      ],
      "metadata": {
        "id": "SJ0iuA0NP6f4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q15. What is causation? Explain difference between correlation and causation with an example."
      ],
      "metadata": {
        "id": "xm_jJifvP_CS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Causation refers to the relationship between two events where one event directly causes the other to happen. It implies a cause-and-effect connection. On the other hand, correlation simply indicates that two events occur together but does not necessarily mean one causes the other. For example, there may be a correlation between the number of ice cream sales and the number of drownings in summer, but it doesn't mean buying ice cream causes drownings. Instead, both are influenced by the warmer weather. Thus, correlation does not always imply causation, as other factors might be at play."
      ],
      "metadata": {
        "id": "GLFlGewoQNUZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q16. What is an Optimizer? What are different types of optimizers? Explain each with an example."
      ],
      "metadata": {
        "id": "ELeDuznKQPKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An optimizer is an algorithm or method used to improve a model's performance by adjusting its parameters to minimize or maximize a specific objective function, commonly used in machine learning and optimization problems. There are several types of optimizers, including gradient descent, which updates parameters based on the gradient of the objective function (e.g., stochastic gradient descent for faster convergence in neural networks). Another example is Adam, which combines the advantages of momentum and adaptive learning rates, making it effective for complex models. Other optimizers include Adagrad and RMSprop, each having specific mechanisms to adjust learning rates during training."
      ],
      "metadata": {
        "id": "XDBxO6MWQSnK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q17. What is sklearn.linear_model ?"
      ],
      "metadata": {
        "id": "HwX2HPWdQg7g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sklearn.linear_model is a module in the scikit-learn library that provides a range of linear models for regression and classification tasks. These models assume a linear relationship between the input features and the target variable. Some common algorithms included in this module are Linear Regression, Logistic Regression, Ridge, Lasso, and ElasticNet. It also supports generalized linear models (GLMs) and allows for regularization to prevent overfitting. These models are widely used in machine learning for tasks such as predicting continuous values or classifying data into categories, depending on the specific algorithm chosen from the module."
      ],
      "metadata": {
        "id": "95ulAneLQnR5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q18. What does model.fit() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "a_3Jx7UjQtW9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `model.fit()` method in machine learning is used to train a model on a given dataset. It adjusts the model's parameters using the training data to minimize the error and improve predictions. The primary arguments required for `model.fit()` include the training data (typically `X_train` for input features) and the corresponding target labels (usually `y_train`). Additionally, optional parameters like `epochs` (number of iterations), `batch_size` (size of data batch per iteration), and validation data (for model evaluation during training) can be provided. It helps the model learn patterns in the dataset for future predictions."
      ],
      "metadata": {
        "id": "9v7cP6hCQw96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q19. What does model.predict() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "td6Sni7pQ3Yd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model.predict() function in machine learning is used to make predictions based on the trained model. It takes input data (such as a set of features or an array) and applies the learned patterns to predict an outcome, such as class labels in classification tasks or continuous values in regression tasks. The primary argument that must be provided is the input data, typically in the form of an array or matrix. Depending on the framework, the format of the input might need to match the model's expected structure (e.g., shape, data type). The function returns the predicted values based on this data."
      ],
      "metadata": {
        "id": "GmtwOv03Q65M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q20. What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "PBdR217fRCPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continuous variables are those that can take an infinite number of values within a given range and are often measured on a scale. They can represent quantities that are divisible, such as height, weight, or time. These variables can be expressed with great precision, using decimals or fractions. On the other hand, categorical variables represent distinct categories or groups and are usually qualitative in nature. They can be nominal, where the categories have no inherent order (e.g., colors or types of fruits), or ordinal, where the categories have a specific order (e.g., rankings or levels of satisfaction)."
      ],
      "metadata": {
        "id": "Jwld2IsaRFoZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q21. What is feature scaling? How does it help in Machine Learning?"
      ],
      "metadata": {
        "id": "KwoPO0qxRNZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature scaling is the process of standardizing or normalizing the range of independent variables or features in a dataset. It ensures that all features contribute equally to the model by transforming them to a similar scale. This is especially important in algorithms like gradient descent, which are sensitive to the scale of features, as well as distance-based algorithms like k-nearest neighbors and support vector machines. Without scaling, features with larger ranges could dominate the model, leading to biased results. By applying feature scaling, the model can learn more effectively and converge faster during training."
      ],
      "metadata": {
        "id": "RS8uAbe7RQqg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q22. How do we perform scaling in Python?"
      ],
      "metadata": {
        "id": "HGg-PjG1RsF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling in Python refers to adjusting the values of numerical data to a specific range or distribution, making it easier for machine learning models to process. The most common methods include normalization and standardization. Normalization typically scales the data between a fixed range, such as 0 and 1, using the min-max scaling technique. Standardization, on the other hand, transforms the data to have a mean of 0 and a standard deviation of 1 using z-score normalization. These techniques can be implemented using libraries like scikit-learn, where functions such as MinMaxScaler or StandardScaler are used to scale the data effectively."
      ],
      "metadata": {
        "id": "0l_Pine7RvVB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q23. What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "At3M8SMrR0CQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling in Python refers to adjusting the values of numerical data to a specific range or distribution, making it easier for machine learning models to process. The most common methods include normalization and standardization. Normalization typically scales the data between a fixed range, such as 0 and 1, using the min-max scaling technique. Standardization, on the other hand, transforms the data to have a mean of 0 and a standard deviation of 1 using z-score normalization. These techniques can be implemented using libraries like scikit-learn, where functions such as MinMaxScaler or StandardScaler are used to scale the data effectively."
      ],
      "metadata": {
        "id": "FnX2kJN-R3rP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q24. How do we split data for model fitting (training and testing) in Python?\n"
      ],
      "metadata": {
        "id": "se3oCBktSA4u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python's `scikit-learn` library provides the `train_test_split` function for partitioning datasets. This function randomly splits data into training and testing sets, allowing you to specify the test set size (or training size) as a proportion of the whole dataset. For example, `train_test_split(data, labels, test_size=0.2)` would create a split where 20% of the data is reserved for testing and 80% is used for training. Using a `random_state` ensures reproducibility of the split across multiple runs. This approach is crucial for evaluating model performance on unseen data.\n"
      ],
      "metadata": {
        "id": "ucs3IOY9SFBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q25. Explain data encoding?"
      ],
      "metadata": {
        "id": "wgxNCsS2SuEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data encoding in machine learning converts categorical data, like text or labels, into numerical representations that algorithms can understand. Many machine learning models operate on numerical input, so encoding is essential for handling non-numerical features. Common methods include one-hot encoding, which creates binary columns for each category, and label encoding, which assigns a unique integer to each category. Proper encoding prevents algorithms from misinterpreting categorical values as ordinal or continuous data, ensuring accurate model training and predictions. Choosing the right encoding technique depends on the nature of the data and the specific machine learning algorithm used.\n"
      ],
      "metadata": {
        "id": "qi_4f2aSSyJJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ggKNKQxEUSeI"
      }
    }
  ]
}